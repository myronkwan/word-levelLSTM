{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wordmodel.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Y5ldWOauK3SM",
        "colab_type": "code",
        "outputId": "34ff8328-f182-44e2-dcd9-0a76815cb0d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "from tensorflow.python.client import device_lib\n",
        "from numpy import array\n",
        "from pickle import dump\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,LSTM,Dropout,Embedding\n",
        "from keras.models import load_model\n",
        "import string \n",
        "import tensorflow as tf\n",
        "import math\n",
        "from collections import OrderedDict\n",
        "import collections\n",
        "import random\n",
        "import sys\n",
        "from google.colab import files\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "faqxErQUknnk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "import nltk\n",
        "from bs4 import BeautifulSoup\n",
        "file=open(\"twotowers.txt\",'w')\n",
        "\n",
        "url='http://ae-lib.org.ua/texts-c/tolkien__the_lord_of_the_rings_1__en.htm'\n",
        "html = urllib.request.urlopen(url)\n",
        "soup=BeautifulSoup(html)\n",
        "\n",
        "for x in soup(['script','style']):\n",
        "  x.extract()\n",
        "\n",
        "text=soup.get_text()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KRVBCD4Pmun0",
        "colab_type": "code",
        "outputId": "ac8e36a9-7147-48cd-a842-05ec00906e91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "text=text[1211:-203].replace('\\n',' ')\n",
        "\n",
        "file.write(text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "995093"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "V-QPxtyfK-Hq",
        "colab_type": "code",
        "outputId": "bc26baee-695c-4a7e-a67c-2510f0c00380",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "inputdata=open('twotowers.txt').read().lower()\n",
        "#remove punctuation and newlines, keep exclamation marks, question marks, periods because they could all signify end of sentence\n",
        "translate_table = dict((ord(char), None) for char in string.punctuation.replace('.','').replace('!','').replace('?','').replace(\"'\",'').replace(',','')) \n",
        "inputdata=inputdata.translate(translate_table).replace('\\n',' ')\n",
        "\n",
        "inputdata=inputdata.replace('\\n',' ').replace('.',' . ').replace('!',' ! ').replace('?',' ? ').replace(\"'\",\" ' \").replace(',',' , ').split()\n",
        "#remove digits\n",
        "for word in inputdata:\n",
        "  if word.isdigit():\n",
        "    inputdata.remove(word)\n",
        "#add spaces between punctuation so we can split by space and treat them as words of their own\n",
        "\n",
        "\n",
        "print(len(inputdata))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "219024\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4P8VHgF0LMgf",
        "colab_type": "code",
        "outputId": "e27d2670-38b8-419e-d744-660d4925e6dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "t=Tokenizer(filters='',char_level=False)\n",
        "t.fit_on_texts(inputdata)\n",
        "#create word dict\n",
        "word2id=t.word_index\n",
        "id2word={v:k for k,v in word2id.items()}\n",
        "\n",
        "vocab_size=len(t.word_index)\n",
        "print(vocab_size)\n",
        "#encode text\n",
        "data=[]\n",
        "for word in inputdata:\n",
        "  data.append(word2id[word])\n",
        "print(len(data))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9672\n",
            "219024\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZvBwLQLMXplf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "input_seq=[]\n",
        "max_n=6 #want max n_gram sequence length to be 6, up to 5 predictors and 1 label\n",
        "#need to pad all sequences to be of at least length max_n\n",
        "\n",
        "\n",
        "for i in range(2,max_n+1):\n",
        "    for j in range(0,len(data)-max_n+1):\n",
        "      input_seq.append(data[j:j+i])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LOHxWA4-eeFZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#pad varied length n_grams\n",
        "input_sequences=pad_sequences(input_seq,maxlen=max_n,padding='pre')\n",
        "predictors,labels=input_sequences[:,:-1],input_sequences[:,-1]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s13mHkDO6x11",
        "colab_type": "code",
        "outputId": "79cc62ce-bc16-4c9b-c812-229806e878ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(predictors.shape,labels.shape)\n",
        "embed_size=128\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1095095, 5) (1095095,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7-Icnh7RETBD",
        "colab_type": "code",
        "outputId": "4bd7138c-74ad-4dec-d2f8-e1ded5815bd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#the output for the embed layer will be a vector of our embed size\n",
        "#https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
        "#define model\n",
        "#https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/\n",
        "model=Sequential()\n",
        "model.add(Embedding(vocab_size+1,embed_size,input_length=max_n-1))\n",
        "model.add(LSTM(700,return_sequences=True))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(LSTM(700,return_sequences=True))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(LSTM(700))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(vocab_size+1,activation='softmax'))\n",
        "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam')\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 5, 128)            1238144   \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 5, 700)            2321200   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 5, 700)            0         \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 5, 700)            3922800   \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 5, 700)            0         \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 700)               3922800   \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 700)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 9673)              6780773   \n",
            "=================================================================\n",
            "Total params: 18,185,717\n",
            "Trainable params: 18,185,717\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YSEJpysJES4V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filepath=\"3layer30.h5\"\n",
        "checkpoint=ModelCheckpoint(filepath,monitor='val_loss',verbose=0,save_weights_only=False)\n",
        "#train model here\n",
        "#model=load_model('2layer100.h5')\n",
        "model.fit(predictors,labels,epochs=20,batch_size=500,callbacks=[checkpoint],verbose=1)\n",
        "model.save('3layer50.h5')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jj-d_l010as5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "files.download('3layer30.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tzVc9zDbZaf0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#load model here, 3 layer lstm trained with 50 epochs of batch size 500\n",
        "model=load_model('3layer50.h5')\n",
        "def generate_text(seed_text, next_words, max_sequence_len, model):\n",
        "    seed_text=seed_text.split()\n",
        "    for i in range(next_words):\n",
        "      if i%25==0:\n",
        "        sys.stdout.write('\\n')\n",
        "      pattern=[]\n",
        "      for word in seed_text:\n",
        "        pattern.append(word2id[word])\n",
        "      pattern=pad_sequences([pattern],maxlen=max_sequence_len-1,dtype='int32',padding='pre',truncating='pre',value=0.0)\n",
        "      prediction=model.predict(pattern,verbose=0)\n",
        "      prediction=np.argmax(prediction)\n",
        "      prediction=id2word[prediction]\n",
        "      sys.stdout.write(prediction+' ')\n",
        "      seed_text.append(prediction)\n",
        "\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7BKkOL7iddXX",
        "colab_type": "code",
        "outputId": "1e9cca53-de6a-4412-f069-4a9c86ab998f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "cell_type": "code",
      "source": [
        "#insert random text as long as it's in the vocabulary\n",
        "generate_text('legolas and the orc',300,max_n,model)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            ", and knew that they were making songs of lamentation for his fall , for they were obliged to put on the ring , whether \n",
            "they think . it was not an orc . the next day the country on either side , and the gate closed silently behind me \n",
            ", and suddenly he felt extremely hungry . the proposal was welcomed by all and they sat down on the upper steps , dim figures \n",
            "in the gloom . ‘don’t you know my name yet ? that’s the only answer . tell me , who are you , and what \n",
            "do you want , and where do they come from , i ' d want it , if you want to know , i will \n",
            "tell you the truth , i was asked to look out for hobbits of the shire , and for a moment he stood trembling in \n",
            "the air . ‘old man willow ? naught worse than that , eh ? that can soon be mended . i have known strong warriors \n",
            "of the big people , though he demanded three guesses . the authorities , it is true , differ whether this last question was a \n",
            "mere ' question ' and not a sound of hoofs in the lane . suddenly as they drew nearer a terrific baying and barking broke \n",
            "out , and his fiftieth birthday was drawing near fifty was a number that he felt was somehow significant or ominous it was at any \n",
            "rate at that age that adventure had suddenly befallen bilbo . frodo began to listen . at first they could see the road sweeping round \n",
            "the feet of the hills , rolling and winding eastward among woods and heathercovered slopes towards the ford and the mountains . he breathed with "
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}